[{"name": "HunsHunsCatB", "date": "05/11/2021 20:44:44", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["bigram_freq", "trigram_freq", "cand_length", "init_word_length", "levenshtein", "freq", "soundex", "metaphone", "keyboard_dist"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.8482849604221636, "acc@3": 0.9802110817941952, "mrr": 0.9141378209155253}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.760027662517289, "acc@3": 0.8208852005532503, "mrr": 0.7905782783376145}}}, {"name": "HunsHunsCatB", "date": "11/11/2021 21:21:28", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["bart_prob", "cand_length_diff", "levenshtein", "freq"]}, "Dataset": "test.bea4k", "Dataset Size": 100, "Ranker Metrics": {"texts_num": 30, "spells_num": 33, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.9696969696969697, "acc@3": 1.0, "mrr": 0.9848484848484849}, "Pipeline Metrics": {"texts_num": 30, "spells_num": 33, "detector_precision": 0.7021276595744681, "detector_recall": 1.0, "candidator_acc (acc@inf)": 0.7878787878787878, "acc@1": 0.7575757575757576, "acc@3": 0.7878787878787878, "mrr": 0.7727272727272727}, "Data Preparation Time": "0:01:44", "Ranker Train Time": "0:00:00", "Ranker Eval Time": "0:00:25", "Pipeline Eval Time": "0:00:48"}}, {"name": "HunsHunsCatB", "date": "05/11/2021 20:37:36", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["cand_length", "init_word_length", "levenshtein", "freq", "soundex", "metaphone", "keyboard_dist"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.8232189973614775, "acc@3": 0.9775725593667546, "mrr": 0.8991037400008375}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7544951590594744, "acc@3": 0.8201936376210235, "mrr": 0.7862790621089376}}}, {"name": "HunsHunsCatB", "date": "05/11/2021 20:40:31", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["bigram_freq", "trigram_freq", "cand_length", "init_word_length", "levenshtein", "freq", "keyboard_dist"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.8225593667546174, "acc@3": 0.9709762532981531, "mrr": 0.8971174384103144}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7461964038727524, "acc@3": 0.8188105117565698, "mrr": 0.7822813597191189}}}, {"name": "HunsHunsCatB", "date": "05/11/2021 20:37:42", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["cand_length", "init_word_length", "levenshtein", "freq", "soundex", "metaphone"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.8192612137203166, "acc@3": 0.9795514511873351, "mrr": 0.8971751057503037}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7420470262793915, "acc@3": 0.8208852005532503, "mrr": 0.7800484094052559}}}, {"name": "HunsHunsCatB", "date": "05/11/2021 20:41:54", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["bigram_freq", "cand_length", "levenshtein", "freq"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.8172823218997362, "acc@3": 0.9676781002638523, "mrr": 0.8929761119487373}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7392807745504841, "acc@3": 0.818118948824343, "mrr": 0.7780801993457594}}}, {"name": "HunsHunsCatB", "date": "03/11/2021 23:48:47", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["levenshtein", "log_freq", "soundex", "cands_less_dist"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.7664907651715039, "acc@3": 0.9736147757255936, "mrr": 0.8680432529212213}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7316735822959889, "acc@3": 0.8174273858921162, "mrr": 0.7743603042876903}}}, {"name": "HunsHunsCatB", "date": "03/11/2021 23:48:53", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["levenshtein", "sqrt_freq", "soundex", "metaphone", "keyboard_dist"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.7862796833773087, "acc@3": 0.9716358839050132, "mrr": 0.8761593507635724}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7233748271092669, "acc@3": 0.8201936376210235, "mrr": 0.7693464730290456}}}, {"name": "HunsHunsCatB", "date": "03/11/2021 23:48:39", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["levenshtein", "freq", "soundex", "metaphone", "keyboard_dist"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.7862796833773087, "acc@3": 0.9716358839050132, "mrr": 0.8761593507635724}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7233748271092669, "acc@3": 0.8201936376210235, "mrr": 0.7693464730290456}}}, {"name": "HunsHunsCatB", "date": "12/11/2021 02:00:00", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["bigram_freq", "trigram_freq", "cand_length_diff", "init_word_length", "levenshtein", "freq", "keyboard_dist"]}, "Dataset": "test.bea4k", "Dataset Size": 200, "Ranker Metrics": {"texts_num": 60, "spells_num": 66, "examples_not_found": [], "examples_not_correct_cands": [{"Text": "So , it would be greatful if you could give us this opportunity to watch the show .\n", "Incorrect Word": "greatful", "Corrected Word": "great", "Candidates": [{"Word": "grateful", "Score": 0.9}, {"Word": "great", "Score": 0.75}, {"Word": "regretful", "Score": 0.18}]}, {"Text": "I AM WRITING WITH REFERENCE TO THE LONDON TRIP WE ARE MAKING PRERY SOON .\n", "Incorrect Word": "PRERY", "Corrected Word": "VERY", "Candidates": [{"Word": "PREY", "Score": 0.28}, {"Word": "VERY", "Score": 0.27}, {"Word": "PERRY", "Score": 0.27}, {"Word": "PRUDERY", "Score": 0.26}]}, {"Text": "IN CASE YOU DECIDE TO CHANGE THE PROGRAMME , WE SUGGEST GOING TO THE SHOW ON TUESDAY AND ON WENSDAY , INSTED OF FREE TIME , VISITING THE SCIENCE MUSEUM .\n", "Incorrect Word": "WENSDAY", "Corrected Word": "WEDNESDAY", "Candidates": [{"Word": "WENS DAY", "Score": 0.28}, {"Word": "WEDNESDAY", "Score": 0.25}, {"Word": "WENS-DAY", "Score": 0.21}]}, {"Text": "I WAS LIVING QUITE CLOUSE TO PORTOBELLO ROAD BUT I DIDN'T KNOW THE AREA VERY WELL BECAUSE ALL THE TIME I JUST HAD TO GET OFF AT THE COURNER OF MY STREET . BUT THAT DAY , AS I WAS KIND OF DRUNK , I DIDN'T REALISE THAT THE BUS TOOK A DIFFERENT ROUTE . SO , WHEN I NOTICED IT , THE DRIVER WAS ALREADY ASKING ME TO GET OFF , BECAUSE WE WERE IN THE GARAGE IN PORTOBELLO .\n", "Incorrect Word": "CLOUSE", "Corrected Word": "CLOSE", "Candidates": [{"Word": "LOUSE", "Score": 0.28}, {"Word": "CLOSE", "Score": 0.28}, {"Word": "CLO USE", "Score": 0.27}, {"Word": "C LOUSE", "Score": 0.27}, {"Word": "CLAUSE", "Score": 0.27}]}, {"Text": "I TOLD HIM I DIDN'T KNOW WHERE I WAS , BUT HE JUST SEID SORRY .\n", "Incorrect Word": "SEID", "Corrected Word": "SAID", "Candidates": [{"Word": "SAD", "Score": 0.29}, {"Word": "SID", "Score": 0.29}, {"Word": "SE ID", "Score": 0.27}, {"Word": "SIDE", "Score": 0.27}, {"Word": "SLID", "Score": 0.27}]}], "candidator_acc (acc@inf)": 1.0, "acc@1": 0.7727272727272727, "acc@3": 0.9696969696969697, "mrr": 0.8683501683501683}, "Pipeline Metrics": {"texts_num": 60, "spells_num": 66, "detector_precision": 0.7738095238095238, "detector_recall": 0.9848484848484849, "examples_not_found": [{"Text": "I thought that Dany Brook , who is my favorite actor , would perform in that show .\n", "Incorrect Word": "favorite", "Corrected Word": "favourite"}], "examples_not_correct_cands": [{"Text": "IN CASE YOU DECIDE TO CHANGE THE PROGRAMME , WE SUGGEST GOING TO THE SHOW ON TUESDAY AND ON WENSDAY , INSTED OF FREE TIME , VISITING THE SCIENCE MUSEUM .\n", "Incorrect Word": "WENSDAY", "Corrected Word": "WEDNESDAY", "Candidates": [{"Word": "WENS DAY", "Score": 0.28}, {"Word": "WEDNESDAY", "Score": 0.25}, {"Word": "WENS-DAY", "Score": 0.21}]}, {"Text": "I WAS LIVING QUITE CLOUSE TO PORTOBELLO ROAD BUT I DIDN'T KNOW THE AREA VERY WELL BECAUSE ALL THE TIME I JUST HAD TO GET OFF AT THE COURNER OF MY STREET . BUT THAT DAY , AS I WAS KIND OF DRUNK , I DIDN'T REALISE THAT THE BUS TOOK A DIFFERENT ROUTE . SO , WHEN I NOTICED IT , THE DRIVER WAS ALREADY ASKING ME TO GET OFF , BECAUSE WE WERE IN THE GARAGE IN PORTOBELLO .\n", "Incorrect Word": "CLOUSE", "Corrected Word": "CLOSE", "Candidates": [{"Word": "LOUSE", "Score": 0.28}, {"Word": "CLOSE", "Score": 0.28}, {"Word": "CLO USE", "Score": 0.27}, {"Word": "C LOUSE", "Score": 0.27}, {"Word": "CLAUSE", "Score": 0.27}]}, {"Text": "I TOLD HIM I DIDN'T KNOW WHERE I WAS , BUT HE JUST SEID SORRY .\n", "Incorrect Word": "SEID", "Corrected Word": "SAID", "Candidates": [{"Word": "SAD", "Score": 0.29}, {"Word": "SID", "Score": 0.29}, {"Word": "SE ID", "Score": 0.27}, {"Word": "SIDE", "Score": 0.27}, {"Word": "SLID", "Score": 0.27}]}, {"Text": "THERE WERE SOME PEOPLE AROUN BUT I WAS AFRAID TO ASK THEM THE WAY .\n", "Incorrect Word": "AROUN", "Corrected Word": "AROUND", "Candidates": [{"Word": "ARON", "Score": 0.28}, {"Word": "HAROUN", "Score": 0.27}, {"Word": "AROUND", "Score": 0.27}]}, {"Text": "AFTER FILLOWING HIM FOR 10 MINUTES , I STARTED TO RECOGNISE THE PLACE , AND I WAS FEELING MORE CONFORTABLE .\n", "Incorrect Word": "CONFORTABLE", "Corrected Word": "COMFORTABLE", "Candidates": [{"Word": "CONFORMABLE", "Score": 0.32}, {"Word": "COMFORTABLE", "Score": 0.32}, {"Word": "CONFERRABLE", "Score": 0.27}, {"Word": "CORRECTABLE", "Score": 0.25}]}], "candidator_acc (acc@inf)": 0.8923076923076924, "acc@1": 0.7230769230769231, "acc@3": 0.8769230769230769, "mrr": 0.794017094017094}, "Data Preparation Time": "0:00:10", "Ranker Train Time": "0:00:00", "Ranker Eval Time": "0:00:01", "Pipeline Eval Time": "0:00:02"}}, {"name": "HunsHunsCatB", "date": "04/11/2021 00:00:15", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["levenshtein", "freq", "soundex", "metaphone"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.762532981530343, "acc@3": 0.9683377308707124, "mrr": 0.8643042826420134}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7226832641770401, "acc@3": 0.8174273858921162, "mrr": 0.7694847856154909}}}, {"name": "HunsHunsCatB", "date": "03/11/2021 23:49:01", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["levenshtein", "freq", "cands_less_dist", "metaphone"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.7420844327176781, "acc@3": 0.9683377308707124, "mrr": 0.8515323323700631}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7102351313969572, "acc@3": 0.8146611341632088, "mrr": 0.7619261674240928}}}, {"name": "HunsHunsCatB", "date": "11/11/2021 17:22:31", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["suffix_prob", "cand_length_diff", "levenshtein", "freq"]}, "Dataset": "test.bea4k", "Ranker Metrics": {"texts_num": 1284, "spells_num": 1500, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.7519788918205804, "acc@3": 0.9656992084432717, "mrr": 0.8577801074255561}, "Pipeline Metrics": {"texts_num": 1284, "spells_num": 1500, "detector_precision": 0.8339100346020761, "detector_recall": 0.964, "candidator_acc (acc@inf)": 0.8278008298755186, "acc@1": 0.7047026279391425, "acc@3": 0.8160442600276625, "mrr": 0.759743078882083}}}, {"name": "HunsHunsCatB", "date": "11/11/2021 21:08:27", "experiment_results": {"Detector": "HunspellDetector", "Candidator": "HunspellCandidator", "Ranker": "CatBoostRanker", "Features": {"RankerFeatures": ["bart_prob", "cand_length_diff", "levenshtein", "freq"]}, "Dataset": "test.bea4k", "Dataset Size": 50, "Ranker Metrics": {"texts_num": 15, "spells_num": 19, "candidator_acc (acc@inf)": 1.0, "acc@1": 0.9473684210526315, "acc@3": 1.0, "mrr": 0.9736842105263158}, "Pipeline Metrics": {"texts_num": 15, "spells_num": 19, "detector_precision": 0.9047619047619048, "detector_recall": 1.0, "candidator_acc (acc@inf)": 0.6842105263157895, "acc@1": 0.631578947368421, "acc@3": 0.6842105263157895, "mrr": 0.6578947368421053}, "Data Preparation Time": "0:00:47", "Ranker Train Time": "0:00:00", "Ranker Eval Time": "0:00:13", "Pipeline Eval Time": "0:00:14"}}]